{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zE-5qhfQAkN"
      },
      "outputs": [],
      "source": [
        "# Applying data augmentation for deep learning model inputs\n",
        "\n",
        "from Bio import SeqIO\n",
        "\n",
        "def has_15_consecutive_common(seq1, seq2):\n",
        "    \"\"\"\n",
        "    Check if two sequences have at least 15 consecutive nucleotides in common.\n",
        "    \"\"\"\n",
        "    for i in range(len(seq1) - 14):  # Check for windows of 15 nucleotides\n",
        "        if seq1[i:i+15] in seq2:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def generate_sequences_with_variable_overlap_and_common_bases(seq, k=40, min_overlap=5, max_overlap=20):\n",
        "    \"\"\"\n",
        "    Generate all possible k-mers (subsequences) of length `k` (40 nucleotides) from the sequence with:\n",
        "    1. Overlaps of varying lengths (5-20 nucleotides).\n",
        "    2. Sequences that share at least 15 consecutive nucleotides with a previously extracted sequence.\n",
        "    \"\"\"\n",
        "    valid_sequences = set()  # Use a set to ensure uniqueness\n",
        "\n",
        "    # 1. Generate subsequences with varying overlaps (5 to 20 nucleotides)\n",
        "    for overlap in range(min_overlap, max_overlap + 1):\n",
        "        # Left overlap\n",
        "        for i in range(0, len(seq) - k + 1, k - overlap):\n",
        "            sub_seq = str(seq[i:i + k])\n",
        "            valid_sequences.add(sub_seq)\n",
        "        # Right overlap\n",
        "        for i in range(overlap, len(seq) - k + 1, k - overlap):\n",
        "            sub_seq = str(seq[i:i + k])\n",
        "            valid_sequences.add(sub_seq)\n",
        "        # Both sides overlap\n",
        "        for i in range(overlap // 2, len(seq) - k + 1, k - overlap):\n",
        "            sub_seq = str(seq[i:i + k])\n",
        "            valid_sequences.add(sub_seq)\n",
        "\n",
        "    # 2. Generate subsequences with at least 15 consecutive nucleotides in common\n",
        "    for i in range(len(seq) - k + 1):\n",
        "        sub_seq = str(seq[i:i + k])\n",
        "        if not valid_sequences:  # If no sequences yet, add the first one\n",
        "            valid_sequences.add(sub_seq)\n",
        "        else:\n",
        "            # Check if the current subsequence has 15 consecutive nucleotides in common with any previously added one\n",
        "            if any(has_15_consecutive_common(existing_seq, sub_seq) for existing_seq in valid_sequences):\n",
        "                valid_sequences.add(sub_seq)\n",
        "\n",
        "    return list(valid_sequences)\n",
        "\n",
        "def process_fasta_for_nn(input_fasta, output_file, k=40, min_overlap=5, max_overlap=20):\n",
        "    \"\"\"\n",
        "    Process each sequence in the input FASTA file to generate subsequences with:\n",
        "    1. Variable overlaps of 5-20 nucleotides.\n",
        "    2. Sequences sharing at least 15 consecutive nucleotides with previous sequences.\n",
        "    Ensure that all subsequences are exactly 40 nucleotides long.\n",
        "    \"\"\"\n",
        "    with open(output_file, 'w') as output_handle:\n",
        "        for record in SeqIO.parse(input_fasta, \"fasta\"):\n",
        "            # Generate subsequences with variable overlap and common bases\n",
        "            overlap_sequences = generate_sequences_with_variable_overlap_and_common_bases(record.seq, k, min_overlap, max_overlap)\n",
        "\n",
        "            # Combine all sequences and ensure uniqueness by using a set\n",
        "            all_sequences = set(overlap_sequences)\n",
        "\n",
        "            label = record.id.split('_')[0]  # Extract label from the header (e.g., \"trnR1\" from \">trnR1_seq_1\")\n",
        "            for sub_seq in all_sequences:\n",
        "                output_handle.write(f\"{sub_seq}, {label}\\n\")\n",
        "\n",
        "# Example usage\n",
        "input_fasta = \"/content/drive/MyDrive/.../chloroplast_300 bp up.fasta\"\n",
        "output_file = \"/content/drive/MyDrive/.../chloroplast_300N_40N_5to20_overlap.csv\"\n",
        "process_fasta_for_nn(input_fasta, output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of CNN-LSTM hybrid model for nucleotide sequences\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# Load and preprocess the data\n",
        "def load_data(file):\n",
        "    df = pd.read_csv(file, header=None)\n",
        "    sequences = df[0].values\n",
        "    labels = df[1].values\n",
        "\n",
        "    # One-hot encode the sequences\n",
        "    def one_hot_encode_sequence(seq):\n",
        "        mapping = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'T': [0, 0, 0, 1]}\n",
        "        return np.array([mapping[nuc] for nuc in seq])\n",
        "\n",
        "    encoded_sequences = np.array([one_hot_encode_sequence(seq) for seq in sequences])\n",
        "    encoded_sequences = encoded_sequences.reshape(len(sequences), -1, 4)  # Reshape to (num_samples, seq_len, num_channels)\n",
        "\n",
        "    # Convert labels to categorical\n",
        "    unique_labels = np.unique(labels)\n",
        "    label_dict = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    encoded_labels = np.array([label_dict[label] for label in labels])\n",
        "\n",
        "    return encoded_sequences, encoded_labels, unique_labels\n",
        "\n",
        "# Build the CNN-LSTM hybrid model\n",
        "class CNNLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(CNNLSTMModel, self).__init__()\n",
        "\n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=0.3, bidirectional=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(hidden_size * 2, 100)  # Bidirectional LSTM doubles hidden size\n",
        "        self.fc2 = nn.Linear(100, 80)\n",
        "        self.output_layer = nn.Linear(80, num_classes)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN forward pass\n",
        "        x = x.permute(0, 2, 1)  # Change to (batch_size, num_channels, seq_len) for Conv1d\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # LSTM forward pass\n",
        "        x = x.permute(0, 2, 1)  # Change to (batch_size, seq_len, num_channels) for LSTM\n",
        "        h0 = torch.zeros(2 * self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)  # Bidirectional LSTM\n",
        "        c0 = torch.zeros(2 * self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))  # Output shape: (batch_size, seq_length, hidden_size*2)\n",
        "\n",
        "        # Take the output from the last time step\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        # Pass through fully connected layers with dropout\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.dropout(out)\n",
        "        out = torch.relu(self.fc2(out))\n",
        "        out = self.output_layer(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Early stopping implementation\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "\n",
        "# Checkpoint functions\n",
        "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(filename, model, optimizer):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(filename)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    return checkpoint['epoch']\n",
        "\n",
        "# Function to evaluate model and return true labels and predicted probabilities\n",
        "def evaluate_model_with_scores(model, data_loader):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    all_true_labels = []\n",
        "    all_pred_scores = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            outputs = model(X_batch)\n",
        "            predicted_probs = torch.softmax(outputs, dim=1)\n",
        "            all_true_labels.extend(y_batch.cpu().numpy())\n",
        "            all_pred_scores.extend(predicted_probs.cpu().numpy())\n",
        "    return np.array(all_true_labels), np.array(all_pred_scores)\n",
        "\n",
        "# Load data\n",
        "input_file = \"/content/drive/MyDrive/..../chloroplast_300N_40N_5to20_overlap.csv\"  # Input file in the format prepared earlier\n",
        "X, y, label_names = load_data(input_file)\n",
        "\n",
        "# Normalize data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X.reshape(X.shape[0], -1)).reshape(X.shape)\n",
        "\n",
        "# Split into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Further split training data for validation (90% training, 10% validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert data to PyTorch tensors and create DataLoader\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
        "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Model parameters\n",
        "input_size = 4  # Since one-hot encoding produces 4 channels for each nucleotide (A, C, G, T)\n",
        "sequence_length = X.shape[1]\n",
        "hidden_size = 128\n",
        "num_layers = 2  # Number of LSTM layers\n",
        "num_classes = len(label_names)\n",
        "\n",
        "# Initialize model, loss function, optimizer, scheduler, and early stopping\n",
        "model = CNNLSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.85)\n",
        "early_stopping = EarlyStopping(patience=8)\n",
        "\n",
        "# Function to train model\n",
        "def train_model(model, train_loader, val_loader, test_loader, num_epochs=50, start_epoch=0):\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        model.train()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "\n",
        "        train_accuracy = 100 * correct / total\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss, correct_val, total_val = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                outputs = model(X_batch)\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct_val += (predicted == y_batch).sum().item()\n",
        "                total_val += y_batch.size(0)\n",
        "\n",
        "        val_accuracy = 100 * correct_val / total_val\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "        early_stopping(avg_val_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "    # Evaluate on test set\n",
        "    model.eval()\n",
        "    test_loss, correct_test, total_test = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_test += (predicted == y_batch).sum().item()\n",
        "            total_test += y_batch.size(0)\n",
        "\n",
        "    test_accuracy = 100 * correct_test / total_test\n",
        "    print(f\"Test Loss: {test_loss/len(test_loader):.4f}, Test Acc: {test_accuracy:.2f}%\")\n",
        "\n",
        "    return model\n",
        "\n",
        "trained_model = train_model(model, train_loader, val_loader, test_loader, num_epochs=50)\n",
        "\n",
        "# Evaluate and generate true labels and predicted scores for precision-recall curve analysis\n",
        "true_labels, pred_scores = evaluate_model_with_scores(trained_model, test_loader)"
      ],
      "metadata": {
        "id": "AcALxrvbQle2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation and performance of CNN-LSTM hybrid model for nucleotide sequences through the average test accuracy (%), calculated across all datasets over multiple trials (5 trials) and epochs\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Load and preprocess the data\n",
        "def load_data(file):\n",
        "    df = pd.read_csv(file, header=None)\n",
        "    sequences = df[0].values\n",
        "    labels = df[1].values\n",
        "\n",
        "    # One-hot encode the sequences\n",
        "    def one_hot_encode_sequence(seq):\n",
        "        mapping = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'T': [0, 0, 0, 1]}\n",
        "        return np.array([mapping[nuc] for nuc in seq])\n",
        "\n",
        "    encoded_sequences = np.array([one_hot_encode_sequence(seq) for seq in sequences])\n",
        "    encoded_sequences = encoded_sequences.reshape(len(sequences), -1, 4)  # Reshape to (num_samples, seq_len, num_channels)\n",
        "\n",
        "    # Convert labels to categorical\n",
        "    unique_labels = np.unique(labels)\n",
        "    label_dict = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    encoded_labels = np.array([label_dict[label] for label in labels])\n",
        "\n",
        "    return encoded_sequences, encoded_labels, unique_labels\n",
        "\n",
        "# Build the CNN-LSTM hybrid model\n",
        "class CNNLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(CNNLSTMModel, self).__init__()\n",
        "\n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=0.3, bidirectional=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(hidden_size * 2, 100)  # Bidirectional LSTM doubles hidden size\n",
        "        self.fc2 = nn.Linear(100, 80)\n",
        "        self.output_layer = nn.Linear(80, num_classes)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN forward pass\n",
        "        x = x.permute(0, 2, 1)  # Change to (batch_size, num_channels, seq_len) for Conv1d\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # LSTM forward pass\n",
        "        x = x.permute(0, 2, 1)  # Change to (batch_size, seq_len, num_channels) for LSTM\n",
        "        h0 = torch.zeros(2 * self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)  # Bidirectional LSTM\n",
        "        c0 = torch.zeros(2 * self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))  # Output shape: (batch_size, seq_length, hidden_size*2)\n",
        "\n",
        "        # Take the output from the last time step\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        # Pass through fully connected layers with dropout\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.dropout(out)\n",
        "        out = torch.relu(self.fc2(out))\n",
        "        out = self.output_layer(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Early stopping implementation\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "\n",
        "# Function to run multiple trials\n",
        "def run_multiple_trials(trials, num_epochs=50):\n",
        "    all_test_accuracies = []\n",
        "    for trial in range(trials):\n",
        "        print(f\"Trial {trial + 1}/{trials}\")\n",
        "\n",
        "        # Load data\n",
        "        input_file = \"/content/drive/MyDrive/..../chloroplast_300N_40N_5to20_overlap.csv\"\n",
        "        X, y, label_names = load_data(input_file)\n",
        "\n",
        "        # Normalize data\n",
        "        scaler = StandardScaler()\n",
        "        X = scaler.fit_transform(X.reshape(X.shape[0], -1)).reshape(X.shape)\n",
        "\n",
        "        # Split into training, validation, and test sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42 + trial)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42 + trial)\n",
        "\n",
        "        # Convert data to PyTorch tensors and create DataLoader\n",
        "        train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "        val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
        "        test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "        # Model parameters\n",
        "        input_size = 4\n",
        "        hidden_size = 128\n",
        "        num_layers = 2\n",
        "        num_classes = len(label_names)\n",
        "\n",
        "        # Initialize model, optimizer, and scheduler\n",
        "        model = CNNLSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "        scheduler = StepLR(optimizer, step_size=5, gamma=0.85)\n",
        "        early_stopping = EarlyStopping(patience=8)\n",
        "\n",
        "        # Train the model\n",
        "        def train_model(num_epochs=50):\n",
        "            for epoch in range(num_epochs):\n",
        "                model.train()\n",
        "                total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "                for X_batch, y_batch in train_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(X_batch)\n",
        "                    loss = criterion(outputs, y_batch)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    total_loss += loss.item()\n",
        "                    _, predicted = torch.max(outputs, 1)\n",
        "                    correct += (predicted == y_batch).sum().item()\n",
        "                    total += y_batch.size(0)\n",
        "\n",
        "                # Validation\n",
        "                model.eval()\n",
        "                val_loss, correct_val, total_val = 0, 0, 0\n",
        "                with torch.no_grad():\n",
        "                    for X_batch, y_batch in val_loader:\n",
        "                        outputs = model(X_batch)\n",
        "                        loss = criterion(outputs, y_batch)\n",
        "                        val_loss += loss.item()\n",
        "                        _, predicted = torch.max(outputs, 1)\n",
        "                        correct_val += (predicted == y_batch).sum().item()\n",
        "                        total_val += y_batch.size(0)\n",
        "\n",
        "                val_accuracy = 100 * correct_val / total_val\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss/len(train_loader):.4f}, \"\n",
        "                      f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "                scheduler.step()\n",
        "\n",
        "                early_stopping(val_loss)\n",
        "                if early_stopping.early_stop:\n",
        "                    print(\"Early stopping\")\n",
        "                    break\n",
        "\n",
        "        train_model()\n",
        "\n",
        "        # Test the model after training\n",
        "        model.eval()\n",
        "        correct_test, total_test = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in test_loader:\n",
        "                outputs = model(X_batch)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct_test += (predicted == y_batch).sum().item()\n",
        "                total_test += y_batch.size(0)\n",
        "\n",
        "        test_accuracy = 100 * correct_test / total_test\n",
        "        print(f\"Trial {trial + 1}/{trials}, Test Accuracy: {test_accuracy:.2f}%\")\n",
        "        all_test_accuracies.append(test_accuracy)\n",
        "\n",
        "    return all_test_accuracies\n",
        "\n",
        "# Running multiple trials\n",
        "results = run_multiple_trials(trials=5)\n",
        "print(\"Final Results:\", results)\n"
      ],
      "metadata": {
        "id": "XPddIzMWRERN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The CNN-LSTM model’s performance via Pearson correlation coefficient\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import os\n",
        "from scipy.stats import pearsonr\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load and preprocess the data\n",
        "def load_data(file):\n",
        "    df = pd.read_csv(file, header=None)\n",
        "    sequences = df[0].values\n",
        "    labels = df[1].values\n",
        "\n",
        "    # One-hot encode the sequences\n",
        "    def one_hot_encode_sequence(seq):\n",
        "        mapping = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'T': [0, 0, 0, 1]}\n",
        "        return np.array([mapping[nuc] for nuc in seq])\n",
        "\n",
        "    encoded_sequences = np.array([one_hot_encode_sequence(seq) for seq in sequences])\n",
        "    encoded_sequences = encoded_sequences.reshape(len(sequences), -1, 4)\n",
        "\n",
        "    # Convert labels to categorical\n",
        "    unique_labels = np.unique(labels)\n",
        "    label_dict = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    encoded_labels = np.array([label_dict[label] for label in labels])\n",
        "\n",
        "    return encoded_sequences, encoded_labels, unique_labels\n",
        "\n",
        "# CNN-LSTM Model\n",
        "class CNNLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(CNNLSTMModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=num_layers,\n",
        "                            batch_first=True, dropout=0.3, bidirectional=True)\n",
        "        self.fc1 = nn.Linear(hidden_size * 2, 100)\n",
        "        self.fc2 = nn.Linear(100, 80)\n",
        "        self.output_layer = nn.Linear(80, num_classes)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        h0 = torch.zeros(2 * self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(2 * self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = out[:, -1, :]\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.dropout(out)\n",
        "        out = torch.relu(self.fc2(out))\n",
        "        out = self.output_layer(out)\n",
        "        return out\n",
        "\n",
        "# Load data\n",
        "input_file = \"/content/drive/MyDrive/..../chloroplast_300N_40N_5to20_overlap.csv\"\n",
        "X, y, label_names = load_data(input_file)\n",
        "\n",
        "# Normalize data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X.reshape(X.shape[0], -1)).reshape(X.shape)\n",
        "\n",
        "# Data splits\n",
        "X_dev, _, y_dev, _ = train_test_split(X, y, test_size=0.9, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "# DataLoader\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
        "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Model parameters\n",
        "input_size = 4\n",
        "sequence_length = X.shape[1]\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = len(label_names)\n",
        "\n",
        "# Model, optimizer, scheduler\n",
        "model = CNNLSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.85)\n",
        "\n",
        "# Evaluation and plotting functions\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct_test, total_test, test_loss = 0, 0, 0\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            correct_test += (predicted == y_batch).sum().item()\n",
        "            total_test += y_batch.size(0)\n",
        "    test_accuracy = 100 * correct_test / total_test\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
        "    return predictions, test_accuracy\n",
        "\n",
        "def calculate_correlation(predictions, true_labels):\n",
        "    correlation, _ = pearsonr(predictions, true_labels)\n",
        "    print(f'Pearson correlation coefficient: {correlation:.4f}')\n",
        "    return correlation\n",
        "\n",
        "# Function to plot the correlation\n",
        "def plot_correlation(predictions, true_labels):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(predictions, [true_labels[i] for i in range(len(true_labels))], alpha=0.6, color='blue', label='Predicted Values')  # Predictions in blue\n",
        "    plt.scatter([true_labels[i] for i in range(len(true_labels))], true_labels, alpha=0.6, color='red', label='Experimental Values')  # Experimental values in red\n",
        "    plt.xlabel('Predicted Values', fontsize=14, labelpad=15)  # Adjust font size\n",
        "    plt.ylabel('Experimental Values', fontsize=14, labelpad=15)  # Adjust font size\n",
        "    plt.title('Arabidopsis thaliana', fontsize=16)  # Title with font size\n",
        "    plt.plot([true_labels.min(), true_labels.max()], [true_labels.min(), true_labels.max()], 'k-', lw=2, label='Perfect Correlation')  # Solid black line\n",
        "    plt.grid(False)\n",
        "    plt.legend(loc='lower right')  # Positioning the legend in the lower right corner\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, test_loader, num_epochs=50):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "        train_accuracy = 100 * correct / total\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss, correct_val, total_val = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                outputs = model(X_batch)\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct_val += (predicted == y_batch).sum().item()\n",
        "                total_val += y_batch.size(0)\n",
        "        val_accuracy = 100 * correct_val / total_val\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
        "        scheduler.step()\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    predictions, test_accuracy = evaluate_model(model, test_loader)\n",
        "    calculate_correlation(predictions, y_test)\n",
        "    plot_correlation(predictions, y_test)\n",
        "\n",
        "# Run training\n",
        "train_model(model, train_loader, val_loader, test_loader, num_epochs=50)\n"
      ],
      "metadata": {
        "id": "FK0cgIP7RTaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The CNN-LSTM model’s performance  via precision-recall curve\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the data\n",
        "def load_data(file):\n",
        "    df = pd.read_csv(file, header=None)\n",
        "    sequences = df[0].values\n",
        "    labels = df[1].values\n",
        "\n",
        "    # One-hot encode the sequences\n",
        "    def one_hot_encode_sequence(seq):\n",
        "        mapping = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'T': [0, 0, 0, 1]}\n",
        "        return np.array([mapping[nuc] for nuc in seq])\n",
        "\n",
        "    encoded_sequences = np.array([one_hot_encode_sequence(seq) for seq in sequences])\n",
        "    encoded_sequences = encoded_sequences.reshape(len(sequences), -1, 4)\n",
        "\n",
        "    # Convert labels to categorical\n",
        "    unique_labels = np.unique(labels)\n",
        "    label_dict = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    encoded_labels = np.array([label_dict[label] for label in labels])\n",
        "\n",
        "    return encoded_sequences, encoded_labels, unique_labels\n",
        "\n",
        "# Build the CNN-LSTM hybrid model\n",
        "class CNNLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(CNNLSTMModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=0.3, bidirectional=True)\n",
        "        self.fc1 = nn.Linear(hidden_size * 2, 100)\n",
        "        self.fc2 = nn.Linear(100, 80)\n",
        "        self.output_layer = nn.Linear(80, num_classes)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        h0 = torch.zeros(2 * self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(2 * self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = out[:, -1, :]\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.dropout(out)\n",
        "        out = torch.relu(self.fc2(out))\n",
        "        out = self.output_layer(out)\n",
        "        return out\n",
        "\n",
        "# Early stopping implementation\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "\n",
        "# Function to evaluate model and return true labels and predicted probabilities\n",
        "def evaluate_model_with_scores(model, data_loader):\n",
        "    model.eval()\n",
        "    all_true_labels = []\n",
        "    all_pred_scores = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            outputs = model(X_batch)\n",
        "            predicted_probs = torch.softmax(outputs, dim=1)\n",
        "            all_true_labels.extend(y_batch.cpu().numpy())\n",
        "            all_pred_scores.extend(predicted_probs.cpu().numpy())\n",
        "    return np.array(all_true_labels), np.array(all_pred_scores)\n",
        "\n",
        "# Load data\n",
        "input_file = \"/content/drive/MyDrive/..../chloroplast_300N_40N_5to20_overlap.csv\"\n",
        "X, y, label_names = load_data(input_file)\n",
        "\n",
        "# Normalize data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X.reshape(X.shape[0], -1)).reshape(X.shape)\n",
        "\n",
        "# Split into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert data to PyTorch tensors and create DataLoader\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
        "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Model parameters\n",
        "input_size = 4\n",
        "sequence_length = X.shape[1]\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = len(label_names)\n",
        "\n",
        "# Initialize model, loss function, optimizer, scheduler, and early stopping\n",
        "model = CNNLSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.85)\n",
        "early_stopping = EarlyStopping(patience=8)\n",
        "\n",
        "# Function to train model\n",
        "def train_model(model, train_loader, val_loader, test_loader, num_epochs=50, start_epoch=0):\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        model.train()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "        train_accuracy = 100 * correct / total\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss, correct_val, total_val = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                outputs = model(X_batch)\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct_val += (predicted == y_batch).sum().item()\n",
        "                total_val += y_batch.size(0)\n",
        "        val_accuracy = 100 * correct_val / total_val\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "        early_stopping(avg_val_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # Test the model\n",
        "    model.eval()\n",
        "    test_loss, correct_test, total_test = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_test += (predicted == y_batch).sum().item()\n",
        "            total_test += y_batch.size(0)\n",
        "\n",
        "    test_accuracy = 100 * correct_test / total_test\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "    print(f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\")\n",
        "\n",
        "    # Precision-recall analysis\n",
        "    true_labels, predicted_probs = evaluate_model_with_scores(model, test_loader)\n",
        "\n",
        "    # Randomly select 10 classes for precision-recall curve\n",
        "    selected_classes = np.random.choice(num_classes, size=10, replace=False)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for i in selected_classes:\n",
        "        precision, recall, _ = precision_recall_curve(true_labels == i, predicted_probs[:, i])\n",
        "        avg_precision = average_precision_score(true_labels == i, predicted_probs[:, i])\n",
        "        auc = np.trapz(precision, recall)  # Calculate the area under the precision-recall curve\n",
        "        plt.plot(recall, precision, label=f'Class {label_names[i]} (AP={avg_precision:.2f}, AUC={auc:.2f})')\n",
        "        # Shade the area under the curve\n",
        "        plt.fill_between(recall, precision, alpha=0.1)\n",
        "\n",
        "    plt.xlabel('Recall', fontsize=14, labelpad=15)\n",
        "    plt.ylabel('Precision', fontsize=14, labelpad=15)\n",
        "    plt.title('Chlamydomonas reinhardtii', fontsize=16)\n",
        "    plt.legend()\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, val_loader, test_loader)\n"
      ],
      "metadata": {
        "id": "1VqpUOcHSoEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The CNN-LSTM model’s performance via Average AUC\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from scipy.stats import sem  # for standard error\n",
        "\n",
        "# Load and preprocess the data\n",
        "def load_data(file):\n",
        "    df = pd.read_csv(file, header=None)\n",
        "    sequences = df[0].values\n",
        "    labels = df[1].values\n",
        "\n",
        "    # One-hot encode the sequences\n",
        "    def one_hot_encode_sequence(seq):\n",
        "        mapping = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'T': [0, 0, 0, 1]}\n",
        "        return np.array([mapping[nuc] for nuc in seq])\n",
        "\n",
        "    encoded_sequences = np.array([one_hot_encode_sequence(seq) for seq in sequences])\n",
        "    encoded_sequences = encoded_sequences.reshape(len(sequences), -1, 4)\n",
        "\n",
        "    # Convert labels to categorical\n",
        "    unique_labels = np.unique(labels)\n",
        "    label_dict = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    encoded_labels = np.array([label_dict[label] for label in labels])\n",
        "\n",
        "    return encoded_sequences, encoded_labels, unique_labels\n",
        "\n",
        "# Build the CNN-LSTM hybrid model\n",
        "class CNNLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(CNNLSTMModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=0.3, bidirectional=True)\n",
        "        self.fc1 = nn.Linear(hidden_size * 2, 100)\n",
        "        self.fc2 = nn.Linear(100, 80)\n",
        "        self.output_layer = nn.Linear(80, num_classes)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        h0 = torch.zeros(2 * self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(2 * self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = out[:, -1, :]\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.dropout(out)\n",
        "        out = torch.relu(self.fc2(out))\n",
        "        out = self.output_layer(out)\n",
        "        return out\n",
        "\n",
        "# Early stopping implementation\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "\n",
        "# Function to evaluate model and return true labels and predicted probabilities\n",
        "def evaluate_model_with_scores(model, data_loader):\n",
        "    model.eval()\n",
        "    all_true_labels = []\n",
        "    all_pred_scores = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            outputs = model(X_batch)\n",
        "            predicted_probs = torch.softmax(outputs, dim=1)\n",
        "            all_true_labels.extend(y_batch.cpu().numpy())\n",
        "            all_pred_scores.extend(predicted_probs.cpu().numpy())\n",
        "    return np.array(all_true_labels), np.array(all_pred_scores)\n",
        "\n",
        "# Load data\n",
        "input_file = \"/content/drive/MyDrive/..../chloroplast_300N_40N_5to20_overlap.csv\"\n",
        "X, y, label_names = load_data(input_file)\n",
        "\n",
        "# Normalize data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X.reshape(X.shape[0], -1)).reshape(X.shape)\n",
        "\n",
        "# Split into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert data to PyTorch tensors and create DataLoader\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
        "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Model parameters\n",
        "input_size = 4\n",
        "sequence_length = X.shape[1]\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = len(label_names)\n",
        "\n",
        "# Initialize model, loss function, optimizer, scheduler, and early stopping\n",
        "model = CNNLSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.85)\n",
        "early_stopping = EarlyStopping(patience=8)\n",
        "\n",
        "# Function to train model\n",
        "def train_model(model, train_loader, val_loader, test_loader, num_epochs=50, start_epoch=0):\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        model.train()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "        train_accuracy = 100 * correct / total\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss, correct_val, total_val = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                outputs = model(X_batch)\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct_val += (predicted == y_batch).sum().item()\n",
        "                total_val += y_batch.size(0)\n",
        "        val_accuracy = 100 * correct_val / total_val\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "        early_stopping(avg_val_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # Test the model\n",
        "    model.eval()\n",
        "    test_loss, correct_test, total_test = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_test += (predicted == y_batch).sum().item()\n",
        "            total_test += y_batch.size(0)\n",
        "\n",
        "    test_accuracy = 100 * correct_test / total_test\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "    print(f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\")\n",
        "\n",
        "    # Precision-recall analysis\n",
        "    true_labels, predicted_probs = evaluate_model_with_scores(model, test_loader)\n",
        "\n",
        "    # Calculate and print AUC for all classes\n",
        "    auc_values = []\n",
        "    for i in range(num_classes):\n",
        "        precision, recall, _ = precision_recall_curve(true_labels == i, predicted_probs[:, i])\n",
        "        auc = average_precision_score(true_labels == i, predicted_probs[:, i])\n",
        "        auc_values.append(auc)\n",
        "        print(f\"AUC for class {label_names[i]}: {auc:.4f}\")\n",
        "\n",
        "    # Print average AUC and standard error\n",
        "    average_auc = np.mean(auc_values)\n",
        "    std_error = sem(auc_values)\n",
        "    print(f\"Average AUC: {average_auc:.4f}, Standard Error: {std_error:.4f}\")\n",
        "\n",
        "# Train and evaluate the model\n",
        "train_model(model, train_loader, val_loader, test_loader)\n"
      ],
      "metadata": {
        "id": "Mh0MMCMJTgqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yXC9bP4yUeUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The CNN-LSTM model’s performance via feature importance analysis utilizing SHAP\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import shap\n",
        "\n",
        "# Load and preprocess the data\n",
        "def load_data(file):\n",
        "    df = pd.read_csv(file, header=None)\n",
        "    sequences = df[0].values\n",
        "    labels = df[1].values\n",
        "\n",
        "    # One-hot encode the sequences\n",
        "    def one_hot_encode_sequence(seq):\n",
        "        mapping = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'T': [0, 0, 0, 1]}\n",
        "        return np.array([mapping[nuc] for nuc in seq])\n",
        "\n",
        "    encoded_sequences = np.array([one_hot_encode_sequence(seq) for seq in sequences])\n",
        "    encoded_sequences = encoded_sequences.reshape(len(sequences), -1, 4)  # Reshape to (num_samples, seq_len, num_channels)\n",
        "\n",
        "    # Convert labels to categorical\n",
        "    unique_labels = np.unique(labels)\n",
        "    label_dict = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    encoded_labels = np.array([label_dict[label] for label in labels])\n",
        "\n",
        "    return encoded_sequences, encoded_labels, unique_labels\n",
        "\n",
        "# Build the CNN-LSTM hybrid model\n",
        "class CNNLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(CNNLSTMModel, self).__init__()\n",
        "\n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=0.3, bidirectional=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(hidden_size * 2, 100)  # Bidirectional LSTM doubles hidden size\n",
        "        self.fc2 = nn.Linear(100, 80)\n",
        "        self.output_layer = nn.Linear(80, num_classes)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN forward pass\n",
        "        x = x.permute(0, 2, 1)  # Change to (batch_size, num_channels, seq_len) for Conv1d\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # LSTM forward pass\n",
        "        x = x.permute(0, 2, 1)  # Change to (batch_size, seq_len, num_channels) for LSTM\n",
        "        h0 = torch.zeros(2 * self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)  # Bidirectional LSTM\n",
        "        c0 = torch.zeros(2 * self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))  # Output shape: (batch_size, seq_length, hidden_size*2)\n",
        "\n",
        "        # Take the output from the last time step\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        # Pass through fully connected layers with dropout\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.dropout(out)\n",
        "        out = torch.relu(self.fc2(out))\n",
        "        out = self.output_layer(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Define the training function\n",
        "def train_model(model, train_loader, val_loader, test_loader, num_epochs):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = StepLR(optimizer, step_size=5, gamma=0.85)\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = correct / total * 100\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "    print('Training complete!')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Feature Importance with SHAP (Updated Version)\n",
        "def calculate_shap_importance(model, X_train, X_test):\n",
        "    # Flatten sequences for SHAP input (samples, features)\n",
        "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    # SHAP Kernel Explainer requires a model's forward function\n",
        "    explainer = shap.KernelExplainer(model_forward_fn, X_train_flat[:10])  # Reduced background data for faster computation\n",
        "    shap_values = explainer.shap_values(X_test_flat[:5])  # Explain fewer samples for speed\n",
        "\n",
        "    # Plot SHAP values for the first instance\n",
        "    plt.figure(dpi=300)  # Set high resolution (DPI: 300)\n",
        "    shap.summary_plot(shap_values, X_test_flat[:5], plot_type=\"bar\")\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig('SHAP Plot_Chlamydomonas reinhardtii.png', bbox_inches='tight', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def model_forward_fn(input_data):\n",
        "    # Reshape the flattened input back to (batch_size, seq_len, num_channels)\n",
        "    input_data = input_data.reshape(input_data.shape[0], -1, 4)\n",
        "    input_tensor = torch.tensor(input_data, dtype=torch.float32)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor).numpy()\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "# Load data\n",
        "input_file = \"/content/drive/MyDrive/..../chloroplast_300N_40N_5to20_overlap.csv\"\n",
        "X, y, label_names = load_data(input_file)\n",
        "\n",
        "# Normalize data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X.reshape(X.shape[0], -1)).reshape(X.shape)\n",
        "\n",
        "# Split into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Model parameters\n",
        "input_size = 4  # Since one-hot encoding produces 4 channels for each nucleotide (A, C, G, T)\n",
        "sequence_length = X.shape[1]\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = len(label_names)\n",
        "\n",
        "# Initialize model\n",
        "model = CNNLSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 5\n",
        "train_model(model, train_loader, None, test_loader, num_epochs)\n",
        "\n",
        "# Run SHAP feature importance analysis\n",
        "calculate_shap_importance(model, X_train, X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "a7yL9EeOUea5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##The CNN-LSTM model’s performance on the protein datasets via average metrics (Macro/Weighted Precision,Recall, and F1 Score) with standard errors\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Data preprocessing function\n",
        "def one_hot_encode_sequence(seq, alphabet=\"ACDEFGHIKLMNPQRSTVWY\"):\n",
        "    encoding = np.zeros((len(seq), len(alphabet)), dtype=np.float32)\n",
        "    for i, aa in enumerate(seq):\n",
        "        if aa in alphabet:\n",
        "            encoding[i, alphabet.index(aa)] = 1.0\n",
        "    return encoding\n",
        "\n",
        "# Custom dataset class\n",
        "class ProteinDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.sequences[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "# Model definition with dropout and an additional dense layer\n",
        "class CNNLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, dropout_rate=0.5):\n",
        "        super(CNNLSTMModel, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(hidden_size, 64)  # Additional dense layer\n",
        "        self.fc2 = nn.Linear(64, num_classes)  # Final layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Change to (batch, channels, sequence_length)\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "\n",
        "        x = x.permute(0, 2, 1)  # Change to (batch, sequence_length, channels)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = x[:, -1, :]  # Take the last hidden state\n",
        "\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))  # Apply dropout before the last layer\n",
        "        out = self.fc2(x)\n",
        "        return out\n",
        "\n",
        "# Load data and preprocess\n",
        "def load_data(file_path):\n",
        "    data = pd.read_csv(file_path, header=None, names=[\"sequence\", \"label\"])\n",
        "    sequences = [one_hot_encode_sequence(seq) for seq in data[\"sequence\"]]\n",
        "    labels = data[\"label\"].astype(\"category\").cat.codes\n",
        "    return np.array(sequences), np.array(labels), data[\"label\"].astype(\"category\").cat.categories\n",
        "\n",
        "# Calculate class weights\n",
        "def calculate_class_weights(labels, num_classes):\n",
        "    label_counts = Counter(labels)\n",
        "    total_samples = sum(label_counts.values())\n",
        "    weights = [total_samples / label_counts[i] if i in label_counts else 0.0 for i in range(num_classes)]\n",
        "    return torch.tensor(weights).to(device)\n",
        "\n",
        "# Optional Focal Loss definition\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = nn.CrossEntropyLoss()(inputs, targets)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "        return focal_loss\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=50):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        for sequences, labels in train_loader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate weighted and macro precision, recall, and F1 scores\n",
        "        precision_macro = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        recall_macro = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "        precision_weighted = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        recall_weighted = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
        "        print(f\"Weighted - Precision: {precision_weighted:.4f}, Recall: {recall_weighted:.4f}, F1 Score: {f1_weighted:.4f}\")\n",
        "        print(f\"Macro - Precision: {precision_macro:.4f}, Recall: {recall_macro:.4f}, F1 Score: {f1_macro:.4f}\")\n",
        "\n",
        "# Evaluation function with confusion matrix\n",
        "def evaluate_model(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        all_preds, all_labels = [], []\n",
        "        total_loss = 0.0\n",
        "        for sequences, labels in data_loader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate weighted and macro precision, recall, and F1 scores\n",
        "        precision_macro = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        recall_macro = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "        precision_weighted = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        recall_weighted = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "        # Print metrics\n",
        "        print(f\"Loss: {total_loss:.4f}\")\n",
        "        print(f\"Weighted - Precision: {precision_weighted:.4f}, Recall: {recall_weighted:.4f}, F1 Score: {f1_weighted:.4f}\")\n",
        "        print(f\"Macro - Precision: {precision_macro:.4f}, Recall: {recall_macro:.4f}, F1 Score: {f1_macro:.4f}\")\n",
        "\n",
        "        return all_labels, all_preds\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Main function with multiple-run Stratified K-Fold Cross-Validation\n",
        "def main():\n",
        "    # Load and prepare data\n",
        "    file_path = '/content/drive/MyDrive/..../chloroplast_40N_5to20_overlap.csv'\n",
        "    sequences, labels, unique_labels = load_data(file_path)\n",
        "\n",
        "    # Parameters for multiple runs\n",
        "    num_runs = 3  # Define the number of runs with different seeds\n",
        "    num_folds = 5  # Define the number of folds for each Stratified K-Fold\n",
        "\n",
        "    # Initialize lists to store performance metrics\n",
        "    precision_macro_list, recall_macro_list, f1_macro_list = [], [], []\n",
        "    precision_weighted_list, recall_weighted_list, f1_weighted_list = [], [], []\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        print(f\"\\n--- Run {run + 1} ---\")\n",
        "\n",
        "        # Set a new seed for each run to vary the split\n",
        "        seed = np.random.randint(10000)\n",
        "        skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
        "\n",
        "        # Store performance metrics for each fold in the current run\n",
        "        run_precision_macro, run_recall_macro, run_f1_macro = [], [], []\n",
        "        run_precision_weighted, run_recall_weighted, run_f1_weighted = [], [], []\n",
        "\n",
        "        for fold, (train_idx, test_idx) in enumerate(skf.split(sequences, labels)):\n",
        "            print(f\"\\nFold {fold + 1} (Seed: {seed})\")\n",
        "\n",
        "            # Split data into training and testing sets\n",
        "            train_sequences, test_sequences = sequences[train_idx], sequences[test_idx]\n",
        "            train_labels, test_labels = labels[train_idx], labels[test_idx]\n",
        "\n",
        "            # Stratified Shuffle Split for validation\n",
        "            sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
        "            for train_idx_val, val_idx in sss.split(train_sequences, train_labels):\n",
        "                train_sequences_split, val_sequences = train_sequences[train_idx_val], train_sequences[val_idx]\n",
        "                train_labels_split, val_labels = train_labels[train_idx_val], train_labels[val_idx]\n",
        "\n",
        "            # Create datasets and dataloaders\n",
        "            train_dataset = ProteinDataset(train_sequences_split, train_labels_split)\n",
        "            val_dataset = ProteinDataset(val_sequences, val_labels)\n",
        "            test_dataset = ProteinDataset(test_sequences, test_labels)\n",
        "\n",
        "            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "            val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "            test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "            # Create model, define loss and optimizer\n",
        "            num_classes = len(unique_labels)\n",
        "            model = CNNLSTMModel(input_size=20, hidden_size=64, num_classes=num_classes).to(device)\n",
        "            criterion = nn.CrossEntropyLoss(weight=calculate_class_weights(train_labels_split, num_classes))\n",
        "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "            # Train the model\n",
        "            train_model(model, train_loader, criterion, optimizer, num_epochs=50)\n",
        "\n",
        "            # Evaluate on validation set\n",
        "            print(\"Validation Set:\")\n",
        "            evaluate_model(model, val_loader, criterion)\n",
        "\n",
        "            # Evaluate on test set and store metrics\n",
        "            print(\"Test Set:\")\n",
        "            test_labels, test_preds = evaluate_model(model, test_loader, criterion)\n",
        "\n",
        "            # Calculate fold metrics\n",
        "            precision_macro = precision_score(test_labels, test_preds, average='macro', zero_division=0)\n",
        "            recall_macro = recall_score(test_labels, test_preds, average='macro', zero_division=0)\n",
        "            f1_macro = f1_score(test_labels, test_preds, average='macro', zero_division=0)\n",
        "\n",
        "            precision_weighted = precision_score(test_labels, test_preds, average='weighted', zero_division=0)\n",
        "            recall_weighted = recall_score(test_labels, test_preds, average='weighted', zero_division=0)\n",
        "            f1_weighted = f1_score(test_labels, test_preds, average='weighted', zero_division=0)\n",
        "\n",
        "            # Append fold metrics to the current run's list\n",
        "            run_precision_macro.append(precision_macro)\n",
        "            run_recall_macro.append(recall_macro)\n",
        "            run_f1_macro.append(f1_macro)\n",
        "            run_precision_weighted.append(precision_weighted)\n",
        "            run_recall_weighted.append(recall_weighted)\n",
        "            run_f1_weighted.append(f1_weighted)\n",
        "\n",
        "        # Store average metrics across folds for the current run\n",
        "        precision_macro_list.append(np.mean(run_precision_macro))\n",
        "        recall_macro_list.append(np.mean(run_recall_macro))\n",
        "        f1_macro_list.append(np.mean(run_f1_macro))\n",
        "        precision_weighted_list.append(np.mean(run_precision_weighted))\n",
        "        recall_weighted_list.append(np.mean(run_recall_weighted))\n",
        "        f1_weighted_list.append(np.mean(run_f1_weighted))\n",
        "\n",
        "    # Calculate final average and standard error across all runs\n",
        "    def mean_std_err(metrics):\n",
        "        return np.mean(metrics), np.std(metrics) / np.sqrt(len(metrics))\n",
        "\n",
        "    # Print average metrics with standard errors\n",
        "    print(\"\\n--- Overall Results ---\")\n",
        "    print(f\"Macro Precision: {mean_std_err(precision_macro_list)}\")\n",
        "    print(f\"Macro Recall: {mean_std_err(recall_macro_list)}\")\n",
        "    print(f\"Macro F1 Score: {mean_std_err(f1_macro_list)}\")\n",
        "    print(f\"Weighted Precision: {mean_std_err(precision_weighted_list)}\")\n",
        "    print(f\"Weighted Recall: {mean_std_err(recall_weighted_list)}\")\n",
        "    print(f\"Weighted F1 Score: {mean_std_err(f1_weighted_list)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "wpdLtAo0U2pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The CNN-LSTM model’s performance on the protein datasets via confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Data preprocessing function\n",
        "def one_hot_encode_sequence(seq, alphabet=\"ACDEFGHIKLMNPQRSTVWY\"):\n",
        "    encoding = np.zeros((len(seq), len(alphabet)), dtype=np.float32)\n",
        "    for i, aa in enumerate(seq):\n",
        "        if aa in alphabet:\n",
        "            encoding[i, alphabet.index(aa)] = 1.0\n",
        "    return encoding\n",
        "\n",
        "# Custom dataset class\n",
        "class ProteinDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.sequences[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "# Model definition with dropout and an additional dense layer\n",
        "class CNNLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, dropout_rate=0.5):\n",
        "        super(CNNLSTMModel, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(hidden_size, 64)  # Additional dense layer\n",
        "        self.fc2 = nn.Linear(64, num_classes)  # Final layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Change to (batch, channels, sequence_length)\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "\n",
        "        x = x.permute(0, 2, 1)  # Change to (batch, sequence_length, channels)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = x[:, -1, :]  # Take the last hidden state\n",
        "\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))  # Apply dropout before the last layer\n",
        "        out = self.fc2(x)\n",
        "        return out\n",
        "\n",
        "# Load data and preprocess\n",
        "def load_data(file_path):\n",
        "    data = pd.read_csv(file_path, header=None, names=[\"sequence\", \"label\"])\n",
        "    sequences = [one_hot_encode_sequence(seq) for seq in data[\"sequence\"]]\n",
        "    labels = data[\"label\"].astype(\"category\").cat.codes\n",
        "    return np.array(sequences), np.array(labels), data[\"label\"].astype(\"category\").cat.categories\n",
        "\n",
        "# Calculate class weights\n",
        "def calculate_class_weights(labels, num_classes):\n",
        "    label_counts = Counter(labels)\n",
        "    total_samples = sum(label_counts.values())\n",
        "    weights = [total_samples / label_counts[i] if i in label_counts else 0.0 for i in range(num_classes)]\n",
        "    return torch.tensor(weights).to(device)\n",
        "\n",
        "# Optional Focal Loss definition\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = nn.CrossEntropyLoss()(inputs, targets)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "        return focal_loss\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=50):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        for sequences, labels in train_loader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate weighted and macro precision, recall, and F1 scores\n",
        "        precision_macro = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        recall_macro = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "        precision_weighted = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        recall_weighted = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
        "        print(f\"Weighted - Precision: {precision_weighted:.4f}, Recall: {recall_weighted:.4f}, F1 Score: {f1_weighted:.4f}\")\n",
        "        print(f\"Macro - Precision: {precision_macro:.4f}, Recall: {recall_macro:.4f}, F1 Score: {f1_macro:.4f}\")\n",
        "\n",
        "# Evaluation function with confusion matrix\n",
        "def evaluate_model(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        all_preds, all_labels = [], []\n",
        "        total_loss = 0.0\n",
        "        for sequences, labels in data_loader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate weighted and macro precision, recall, and F1 scores\n",
        "        precision_macro = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        recall_macro = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "        precision_weighted = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        recall_weighted = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "        # Print metrics\n",
        "        print(f\"Loss: {total_loss:.4f}\")\n",
        "        print(f\"Weighted - Precision: {precision_weighted:.4f}, Recall: {recall_weighted:.4f}, F1 Score: {f1_weighted:.4f}\")\n",
        "        print(f\"Macro - Precision: {precision_macro:.4f}, Recall: {recall_macro:.4f}, F1 Score: {f1_macro:.4f}\")\n",
        "\n",
        "        return all_labels, all_preds\n",
        "\n",
        "# Main function to prepare data, create the model, and train/test with Stratified K-Fold\n",
        "def main():\n",
        "    # Load and prepare data\n",
        "    file_path = '/content/drive/MyDrive/..../chloroplast_40N_5to20_overlap.csv'\n",
        "    sequences, labels, unique_labels = load_data(file_path)\n",
        "\n",
        "    # Stratified K-Fold Cross-Validation\n",
        "    skf = StratifiedKFold(n_splits=5)\n",
        "    overall_cm = np.zeros((len(unique_labels), len(unique_labels)), dtype=int)\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(skf.split(sequences, labels)):\n",
        "        print(f\"Fold {fold + 1}\")\n",
        "\n",
        "        # Split data into training and testing sets\n",
        "        train_sequences, test_sequences = sequences[train_idx], sequences[test_idx]\n",
        "        train_labels, test_labels = labels[train_idx], labels[test_idx]\n",
        "\n",
        "        # Stratified Shuffle Split for validation\n",
        "        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "        for train_idx_val, val_idx in sss.split(train_sequences, train_labels):\n",
        "            train_sequences_split, val_sequences = train_sequences[train_idx_val], train_sequences[val_idx]\n",
        "            train_labels_split, val_labels = train_labels[train_idx_val], train_labels[val_idx]\n",
        "\n",
        "        # Create datasets and dataloaders\n",
        "        train_dataset = ProteinDataset(train_sequences_split, train_labels_split)\n",
        "        val_dataset = ProteinDataset(val_sequences, val_labels)\n",
        "        test_dataset = ProteinDataset(test_sequences, test_labels)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "        # Create model, define loss and optimizer\n",
        "        num_classes = len(unique_labels)\n",
        "        model = CNNLSTMModel(input_size=20, hidden_size=64, num_classes=num_classes).to(device)\n",
        "        criterion = nn.CrossEntropyLoss(weight=calculate_class_weights(train_labels_split, num_classes))\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        # Train the model\n",
        "        train_model(model, train_loader, criterion, optimizer, num_epochs=50)\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        print(\"Validation Set:\")\n",
        "        val_labels, val_preds = evaluate_model(model, val_loader, criterion)\n",
        "\n",
        "        # Evaluate on test set and accumulate confusion matrix\n",
        "        print(\"Test Set:\")\n",
        "        test_labels, test_preds = evaluate_model(model, test_loader, criterion)\n",
        "        cm = confusion_matrix(test_labels, test_preds, labels=np.arange(num_classes))\n",
        "        overall_cm += cm\n",
        "\n",
        "\n",
        "    # Final confusion matrix heatmap\n",
        "    plt.figure(figsize=(14, 12))\n",
        "    # Ensure the confusion matrix includes all labels\n",
        "    sns.heatmap(overall_cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=unique_labels, yticklabels=unique_labels,\n",
        "            cbar_kws={'label': 'Number of Predictions'})\n",
        "    plt.ylabel('True Label', fontsize=16, labelpad=15)\n",
        "    plt.xlabel('Predicted Label', fontsize=16, labelpad=15)\n",
        "    plt.title('Arabidopsis thaliana', fontsize=18)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('Arabidopsis thaliana_confusion_matrix.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "0ASQ319xU29B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##The CNN-LSTM model’s performance on the protein datasets via Training and validation performance\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt  # Importing matplotlib for plotting\n",
        "\n",
        "# Load and preprocess the data\n",
        "def load_data(file):\n",
        "    df = pd.read_csv(file, header=None)\n",
        "    sequences = df[0].values\n",
        "    labels = df[1].values\n",
        "\n",
        "    # One-hot encode the sequences\n",
        "    def one_hot_encode_sequence(seq):\n",
        "        mapping = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'T': [0, 0, 0, 1]}\n",
        "        return np.array([mapping[nuc] for nuc in seq])\n",
        "\n",
        "    encoded_sequences = np.array([one_hot_encode_sequence(seq) for seq in sequences])\n",
        "    encoded_sequences = encoded_sequences.reshape(len(sequences), -1, 4)  # Reshape to (num_samples, seq_len, num_channels)\n",
        "\n",
        "    # Convert labels to categorical\n",
        "    unique_labels = np.unique(labels)\n",
        "    label_dict = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    encoded_labels = np.array([label_dict[label] for label in labels])\n",
        "\n",
        "    return encoded_sequences, encoded_labels, unique_labels\n",
        "\n",
        "# Build the CNN-LSTM hybrid model\n",
        "class CNNLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(CNNLSTMModel, self).__init__()\n",
        "\n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=0.3, bidirectional=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(hidden_size * 2, 100)  # Bidirectional LSTM doubles hidden size\n",
        "        self.fc2 = nn.Linear(100, 80)\n",
        "        self.output_layer = nn.Linear(80, num_classes)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN forward pass\n",
        "        x = x.permute(0, 2, 1)  # Change to (batch_size, num_channels, seq_len) for Conv1d\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # LSTM forward pass\n",
        "        x = x.permute(0, 2, 1)  # Change to (batch_size, seq_len, num_channels) for LSTM\n",
        "        h0 = torch.zeros(2 * self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)  # Bidirectional LSTM\n",
        "        c0 = torch.zeros(2 * self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))  # Output shape: (batch_size, seq_length, hidden_size*2)\n",
        "\n",
        "        # Take the output from the last time step\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        # Pass through fully connected layers with dropout\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.dropout(out)\n",
        "        out = torch.relu(self.fc2(out))\n",
        "        out = self.output_layer(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Early stopping implementation\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "\n",
        "# Checkpoint functions\n",
        "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(filename, model, optimizer):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(filename)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    return checkpoint['epoch']\n",
        "\n",
        "# Function to evaluate model and return true labels and predicted probabilities\n",
        "def evaluate_model_with_scores(model, data_loader):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    all_true_labels = []\n",
        "    all_pred_scores = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            outputs = model(X_batch)\n",
        "            predicted_probs = torch.softmax(outputs, dim=1)\n",
        "            all_true_labels.extend(y_batch.cpu().numpy())\n",
        "            all_pred_scores.extend(predicted_probs.cpu().numpy())\n",
        "    return np.array(all_true_labels), np.array(all_pred_scores)\n",
        "\n",
        "# Load data\n",
        "input_file = \"/content/drive/MyDrive/..../chloroplast_300 bp up.csv\"  # Input file in the format prepared earlier\n",
        "X, y, label_names = load_data(input_file)\n",
        "\n",
        "# Normalize data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X.reshape(X.shape[0], -1)).reshape(X.shape)\n",
        "\n",
        "# Split into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Further split training data for validation (90% training, 10% validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert data to PyTorch tensors and create DataLoader\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
        "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Model parameters\n",
        "input_size = 4  # Since one-hot encoding produces 4 channels for each nucleotide (A, C, G, T)\n",
        "sequence_length = X.shape[1]\n",
        "hidden_size = 128\n",
        "num_layers = 2  # Number of LSTM layers\n",
        "num_classes = len(label_names)\n",
        "\n",
        "# Initialize model, loss function, optimizer, scheduler, and early stopping\n",
        "model = CNNLSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.85)\n",
        "early_stopping = EarlyStopping(patience=8)\n",
        "\n",
        "# Function to train model\n",
        "def train_model(model, train_loader, val_loader, test_loader, num_epochs=50, start_epoch=0):\n",
        "    # Lists to store losses and accuracies\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        model.train()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "\n",
        "        train_accuracy = 100 * correct / total\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss, correct_val, total_val = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                outputs = model(X_batch)\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct_val += (predicted == y_batch).sum().item()\n",
        "                total_val += y_batch.size(0)\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_accuracy = 100 * correct_val / total_val\n",
        "        val_losses.append(avg_val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}')\n",
        "\n",
        "        # Learning rate scheduler step\n",
        "        scheduler.step()\n",
        "\n",
        "        # Early stopping check\n",
        "        early_stopping(avg_val_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "    # After training, evaluate model performance on the test set\n",
        "    true_labels, pred_scores = evaluate_model_with_scores(model, test_loader)\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
        "\n",
        "# Train the model\n",
        "train_losses, val_losses, train_accuracies, val_accuracies = train_model(model, train_loader, val_loader, test_loader)\n",
        "\n",
        "# Plot training and validation loss and accuracy in a single plot\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Plot loss on the primary y-axis (left side)\n",
        "train_loss_line, = ax1.plot(train_losses, label='Training Loss', color='blue', marker='o')\n",
        "val_loss_line, = ax1.plot(val_losses, label='Validation Loss', color='orange', marker='x')\n",
        "\n",
        "# Label for the left y-axis (Loss)\n",
        "ax1.set_ylabel('Loss', fontsize=16, labelpad=15)\n",
        "ax1.set_xlabel('Epochs', fontsize=16, labelpad=15)\n",
        "\n",
        "# Create a second y-axis for accuracy\n",
        "ax2 = ax1.twinx()\n",
        "train_accuracy_line, = ax2.plot(train_accuracies, label='Training Accuracy', color='green', marker='s')\n",
        "val_accuracy_line, = ax2.plot(val_accuracies, label='Validation Accuracy', color='red', marker='d')\n",
        "\n",
        "# Label for the right y-axis (Accuracy)\n",
        "ax2.set_ylabel('Accuracy (%)', fontsize=16, labelpad=15)\n",
        "\n",
        "# Title for the plot\n",
        "plt.title('Chlamydomonas reinhardtii', fontsize=18)\n",
        "\n",
        "# Combine legends - avoiding duplicate accuracy entries\n",
        "lines = [train_loss_line, val_loss_line, train_accuracy_line, val_accuracy_line]\n",
        "labels = ['Training Loss', 'Validation Loss', 'Training Accuracy', 'Validation Accuracy']\n",
        "ax1.legend(lines, labels, loc='upper center', bbox_to_anchor=(0.87, 0.6))\n",
        "\n",
        "# Save the plot in high resolution\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"training_validation_plot_Chlamydomonas reinhardtii_no.png\", dpi=300, bbox_inches='tight')  # Save as PNG with 300 DPI\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Nhu98LFJrenx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The CNN-LSTM model’s performance on the protein datasets via Loss,Recall & F1 Score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Data preprocessing function\n",
        "def one_hot_encode_sequence(seq, alphabet=\"ACDEFGHIKLMNPQRSTVWY\"):\n",
        "    encoding = np.zeros((len(seq), len(alphabet)), dtype=np.float32)\n",
        "    for i, aa in enumerate(seq):\n",
        "        if aa in alphabet:\n",
        "            encoding[i, alphabet.index(aa)] = 1.0\n",
        "    return encoding\n",
        "\n",
        "# Custom dataset class\n",
        "class ProteinDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.sequences[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "# Model definition with dropout and an additional dense layer\n",
        "class CNNLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, dropout_rate=0.5):\n",
        "        super(CNNLSTMModel, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(hidden_size, 64)  # Additional dense layer\n",
        "        self.fc2 = nn.Linear(64, num_classes)  # Final layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Change to (batch, channels, sequence_length)\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "\n",
        "        x = x.permute(0, 2, 1)  # Change to (batch, sequence_length, channels)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = x[:, -1, :]  # Take the last hidden state\n",
        "\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))  # Apply dropout before the last layer\n",
        "        out = self.fc2(x)\n",
        "        return out\n",
        "\n",
        "# Load data and preprocess\n",
        "def load_data(file_path):\n",
        "    data = pd.read_csv(file_path, header=None, names=[\"sequence\", \"label\"])\n",
        "    sequences = [one_hot_encode_sequence(seq) for seq in data[\"sequence\"]]\n",
        "    labels = data[\"label\"].astype(\"category\").cat.codes\n",
        "    return np.array(sequences), np.array(labels), data[\"label\"].astype(\"category\").cat.categories\n",
        "\n",
        "# Calculate class weights\n",
        "def calculate_class_weights(labels, num_classes):\n",
        "    label_counts = Counter(labels)\n",
        "    total_samples = sum(label_counts.values())\n",
        "    weights = [total_samples / label_counts[i] if i in label_counts else 0.0 for i in range(num_classes)]\n",
        "    return torch.tensor(weights).to(device)\n",
        "\n",
        "# Optional Focal Loss definition\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = nn.CrossEntropyLoss()(inputs, targets)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "        return focal_loss\n",
        "\n",
        "# Training function\n",
        "from sklearn.metrics import f1_score, recall_score\n",
        "\n",
        "# Initialize lists to store metrics for each epoch across all folds\n",
        "train_losses_folds = []\n",
        "val_losses_folds = []\n",
        "train_f1_scores_folds = []\n",
        "val_f1_scores_folds = []\n",
        "train_recalls_folds = []\n",
        "val_recalls_folds = []\n",
        "\n",
        "# Training function (modified to return metrics for each fold)\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
        "    fold_train_losses = []\n",
        "    fold_val_losses = []\n",
        "    fold_train_f1_scores = []\n",
        "    fold_val_f1_scores = []\n",
        "    fold_train_recalls = []\n",
        "    fold_val_recalls = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training loop\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        all_train_preds, all_train_labels = [], []\n",
        "        for sequences, labels in train_loader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_train_preds.extend(preds.cpu().numpy())\n",
        "            all_train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate training metrics\n",
        "        train_loss = total_loss / len(train_loader)\n",
        "        train_f1 = f1_score(all_train_labels, all_train_preds, average='weighted', zero_division=0)\n",
        "        train_recall = recall_score(all_train_labels, all_train_preds, average='weighted', zero_division=0)\n",
        "\n",
        "        fold_train_losses.append(train_loss)\n",
        "        fold_train_f1_scores.append(train_f1)\n",
        "        fold_train_recalls.append(train_recall)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0.0\n",
        "            all_val_preds, all_val_labels = [], []\n",
        "            for sequences, labels in val_loader:\n",
        "                sequences, labels = sequences.to(device), labels.to(device)\n",
        "                outputs = model(sequences)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                all_val_preds.extend(preds.cpu().numpy())\n",
        "                all_val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Calculate validation metrics\n",
        "            val_loss /= len(val_loader)\n",
        "            val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
        "            val_recall = recall_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
        "\n",
        "            fold_val_losses.append(val_loss)\n",
        "            fold_val_f1_scores.append(val_f1)\n",
        "            fold_val_recalls.append(val_recall)\n",
        "\n",
        "        # Print metrics for each epoch\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "        print(f\"Train F1 Score: {train_f1:.4f}, Val F1 Score: {val_f1:.4f}\")\n",
        "        print(f\"Train Recall: {train_recall:.4f}, Val Recall: {val_recall:.4f}\")\n",
        "\n",
        "    # Return metrics for the current fold\n",
        "    return fold_train_losses, fold_val_losses, fold_train_f1_scores, fold_val_f1_scores, fold_train_recalls, fold_val_recalls\n",
        "\n",
        "# Main function to prepare data, create the model, and train/test with Stratified K-Fold\n",
        "def main():\n",
        "    # Load and prepare data\n",
        "    file_path = '/content/drive/MyDrive/..../chloroplast_40N_5to20_overlap.csv'\n",
        "    sequences, labels, unique_labels = load_data(file_path)\n",
        "\n",
        "    # Stratified K-Fold Cross-Validation\n",
        "    skf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(skf.split(sequences, labels)):\n",
        "        print(f\"Fold {fold + 1}\")\n",
        "\n",
        "        # Split data into training and testing sets\n",
        "        train_sequences, test_sequences = sequences[train_idx], sequences[test_idx]\n",
        "        train_labels, test_labels = labels[train_idx], labels[test_idx]\n",
        "\n",
        "        # Stratified Shuffle Split for validation\n",
        "        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "        for train_idx_val, val_idx in sss.split(train_sequences, train_labels):\n",
        "            train_sequences_split, val_sequences = train_sequences[train_idx_val], train_sequences[val_idx]\n",
        "            train_labels_split, val_labels = train_labels[train_idx_val], train_labels[val_idx]\n",
        "\n",
        "        # Create datasets and dataloaders\n",
        "        train_dataset = ProteinDataset(train_sequences_split, train_labels_split)\n",
        "        val_dataset = ProteinDataset(val_sequences, val_labels)\n",
        "        test_dataset = ProteinDataset(test_sequences, test_labels)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "        # Create model, define loss and optimizer\n",
        "        model = CNNLSTMModel(input_size=20, hidden_size=64, num_classes=len(unique_labels)).to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        # Train model and get metrics\n",
        "        train_losses, val_losses, train_f1s, val_f1s, train_recalls, val_recalls = train_model(\n",
        "            model, train_loader, val_loader, criterion, optimizer, num_epochs=50\n",
        "        )\n",
        "\n",
        "        # Store metrics for each fold\n",
        "        train_losses_folds.append(train_losses)\n",
        "        val_losses_folds.append(val_losses)\n",
        "        train_f1_scores_folds.append(train_f1s)\n",
        "        val_f1_scores_folds.append(val_f1s)\n",
        "        train_recalls_folds.append(train_recalls)\n",
        "        val_recalls_folds.append(val_recalls)\n",
        "\n",
        "# Run main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# Plotting the metrics\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Create first y-axis for loss\n",
        "ax1 = plt.gca()\n",
        "ax1.set_xlabel(\"Epochs\", fontsize=14, labelpad=15)\n",
        "ax1.set_ylabel(\"Loss\", color='tab:red', fontsize=14, labelpad=15)\n",
        "\n",
        "# Calculate mean and std for train and validation losses\n",
        "mean_train_losses = np.mean(train_losses_folds, axis=0)\n",
        "mean_val_losses = np.mean(val_losses_folds, axis=0)\n",
        "std_train_losses = np.std(train_losses_folds, axis=0)\n",
        "std_val_losses = np.std(val_losses_folds, axis=0)\n",
        "\n",
        "# Plot mean loss with shaded areas for std deviation\n",
        "ax1.plot(mean_train_losses, label='Mean Train Loss', color='tab:red', linestyle='--')\n",
        "ax1.plot(mean_val_losses, label='Mean Val Loss', color='tab:red')\n",
        "\n",
        "# Fill between for standard deviation\n",
        "ax1.fill_between(range(len(mean_train_losses)),\n",
        "                 mean_train_losses - std_train_losses,\n",
        "                 mean_train_losses + std_train_losses,\n",
        "                 color='tab:red', alpha=0.2)\n",
        "ax1.fill_between(range(len(mean_val_losses)),\n",
        "                 mean_val_losses - std_val_losses,\n",
        "                 mean_val_losses + std_val_losses,\n",
        "                 color='tab:red', alpha=0.2)\n",
        "\n",
        "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
        "\n",
        "# Create second y-axis for recall and F1 score\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel(\"Recall & F1 Score\", color='tab:blue', fontsize=14, labelpad=15)\n",
        "\n",
        "# Calculate mean and std for F1 scores and recalls\n",
        "mean_train_f1s = np.mean(train_f1_scores_folds, axis=0)\n",
        "mean_val_f1s = np.mean(val_f1_scores_folds, axis=0)\n",
        "mean_train_recalls = np.mean(train_recalls_folds, axis=0)\n",
        "mean_val_recalls = np.mean(val_recalls_folds, axis=0)\n",
        "\n",
        "std_train_f1s = np.std(train_f1_scores_folds, axis=0)\n",
        "std_val_f1s = np.std(val_f1_scores_folds, axis=0)\n",
        "std_train_recalls = np.std(train_recalls_folds, axis=0)\n",
        "std_val_recalls = np.std(val_recalls_folds, axis=0)\n",
        "\n",
        "# Plot mean F1 scores and recalls with shaded areas for std deviation\n",
        "ax2.plot(mean_train_f1s, label='Mean Train F1 Score', color='tab:blue', linestyle='--')\n",
        "ax2.plot(mean_val_f1s, label='Mean Val F1 Score', color='tab:blue')\n",
        "ax2.fill_between(range(len(mean_train_f1s)),\n",
        "                 mean_train_f1s - std_train_f1s,\n",
        "                 mean_train_f1s + std_train_f1s,\n",
        "                 color='tab:blue', alpha=0.2)\n",
        "ax2.fill_between(range(len(mean_val_f1s)),\n",
        "                 mean_val_f1s - std_val_f1s,\n",
        "                 mean_val_f1s + std_val_f1s,\n",
        "                 color='tab:blue', alpha=0.2)\n",
        "\n",
        "ax2.plot(mean_train_recalls, label='Mean Train Recall', color='tab:green', linestyle='--')\n",
        "ax2.plot(mean_val_recalls, label='Mean Val Recall', color='tab:green')\n",
        "ax2.fill_between(range(len(mean_train_recalls)),\n",
        "                 mean_train_recalls - std_train_recalls,\n",
        "                 mean_train_recalls + std_train_recalls,\n",
        "                 color='tab:green', alpha=0.2)\n",
        "ax2.fill_between(range(len(mean_val_recalls)),\n",
        "                 mean_val_recalls - std_val_recalls,\n",
        "                 mean_val_recalls + std_val_recalls,\n",
        "                 color='tab:green', alpha=0.2)\n",
        "\n",
        "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
        "\n",
        "# Add title and legend\n",
        "plt.title(\"Arabidopsis thaliana\", fontsize=16)\n",
        "\n",
        "# Adjust legend position\n",
        "ax1.legend(loc='center right', bbox_to_anchor=(1, 0.2), framealpha=0.5)\n",
        "ax2.legend(loc='center right', bbox_to_anchor=(1, 0.75), framealpha=0.5)\n",
        "\n",
        "# Save the plot as a high-resolution image\n",
        "plt.savefig(\"Arabidopsis thaliana_training_validation_metrics.png\", dpi=300, bbox_inches='tight')\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "sMZl_RaRV-IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The CNN-LSTM model’s performance on the protein datasets via ROC Curve & Precision-Recallcurves\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve, roc_curve, auc\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Data preprocessing function\n",
        "def one_hot_encode_sequence(seq, alphabet=\"ACDEFGHIKLMNPQRSTVWY\"):\n",
        "    encoding = np.zeros((len(seq), len(alphabet)), dtype=np.float32)\n",
        "    for i, aa in enumerate(seq):\n",
        "        if aa in alphabet:\n",
        "            encoding[i, alphabet.index(aa)] = 1.0\n",
        "    return encoding\n",
        "\n",
        "# Custom dataset class\n",
        "class ProteinDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.sequences[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "# Model definition with dropout and an additional dense layer\n",
        "class CNNLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, dropout_rate=0.5):\n",
        "        super(CNNLSTMModel, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(hidden_size, 64)  # Additional dense layer\n",
        "        self.fc2 = nn.Linear(64, num_classes)  # Final layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Change to (batch, channels, sequence_length)\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "\n",
        "        x = x.permute(0, 2, 1)  # Change to (batch, sequence_length, channels)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = x[:, -1, :]  # Take the last hidden state\n",
        "\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))  # Apply dropout before the last layer\n",
        "        out = self.fc2(x)\n",
        "        return out\n",
        "\n",
        "# Load data and preprocess\n",
        "def load_data(file_path):\n",
        "    data = pd.read_csv(file_path, header=None, names=[\"sequence\", \"label\"])\n",
        "    sequences = [one_hot_encode_sequence(seq) for seq in data[\"sequence\"]]\n",
        "    labels = data[\"label\"].astype(\"category\").cat.codes\n",
        "    return np.array(sequences), np.array(labels), data[\"label\"].astype(\"category\").cat.categories\n",
        "\n",
        "\n",
        "# Calculate class weights\n",
        "def calculate_class_weights(labels, num_classes):\n",
        "    label_counts = Counter(labels)\n",
        "    total_samples = sum(label_counts.values())\n",
        "    weights = [total_samples / label_counts[i] if i in label_counts else 0.0 for i in range(num_classes)]\n",
        "    return torch.tensor(weights).to(device)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=50):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for sequences, labels in train_loader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Evaluation function with Precision-Recall and ROC curves\n",
        "def evaluate_model(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    all_probs = []  # Collect predicted probabilities\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in data_loader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(torch.softmax(outputs, dim=1).cpu().numpy())  # Get probabilities\n",
        "\n",
        "        # Convert lists to numpy arrays\n",
        "        all_labels = np.array(all_labels)\n",
        "        all_preds = np.array(all_preds)\n",
        "        all_probs = np.array(all_probs)\n",
        "\n",
        "        # Calculate metrics with zero_division parameter\n",
        "        weighted_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        weighted_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        weighted_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "        macro_precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        macro_recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "        print(f'Weighted Precision: {weighted_precision:.4f}, Weighted Recall: {weighted_recall:.4f}, Weighted F1: {weighted_f1:.4f}')\n",
        "        print(f'Macro Precision: {macro_precision:.4f}, Macro Recall: {macro_recall:.4f}, Macro F1: {macro_f1:.4f}')\n",
        "\n",
        "        return all_labels, all_preds, np.array(all_probs), total_loss  # Return probabilities for AUC\n",
        "\n",
        "\n",
        "# Plot Precision-Recall and ROC curves\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Precision-Recall and ROC curves with highlighted AUC\n",
        "def plot_metrics(y_true, y_scores, num_classes):\n",
        "    # Calculate and plot average Precision-Recall curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    average_precision = []\n",
        "    average_auc_pr = 0\n",
        "    auc_pr_list = []\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        precision, recall, _ = precision_recall_curve(y_true, y_scores[:, i], pos_label=i)\n",
        "        auc_pr = auc(recall, precision)\n",
        "        auc_pr_list.append(auc_pr)\n",
        "        average_auc_pr += auc_pr\n",
        "        plt.plot(recall, precision, label=f'Class {i} (AUC = {auc_pr:.2f})')\n",
        "\n",
        "    average_auc_pr /= num_classes\n",
        "    std_error_pr = np.std(auc_pr_list) / np.sqrt(num_classes)\n",
        "    print(f\"Average AUC for Precision-Recall Curve: {average_auc_pr:.2f} ± {std_error_pr:.2f}\")\n",
        "\n",
        "    plt.title('Precision-Recall Curve - Arabidopsis thaliana', fontsize=16)\n",
        "    plt.xlabel('Recall', fontsize=14, labelpad=15)\n",
        "    plt.ylabel('Precision', fontsize=14, labelpad=15)\n",
        "    plt.grid(False)\n",
        "    plt.text(0.6, 0.1, f\"Avg AUC = {average_auc_pr:.2f} ± {std_error_pr:.2f}\",\n",
        "             transform=plt.gca().transAxes, fontsize=12, color='darkblue')  # AUC annotation\n",
        "    plt.savefig(\"Arabidopsis_thaliana_precision_recall_curve.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate and plot average ROC curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    average_auc_roc = 0\n",
        "    auc_roc_list = []\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_scores[:, i], pos_label=i)\n",
        "        auc_roc = auc(fpr, tpr)\n",
        "        auc_roc_list.append(auc_roc)\n",
        "        average_auc_roc += auc_roc\n",
        "        plt.plot(fpr, tpr, label=f'Class {i} (AUC = {auc_roc:.2f})')\n",
        "\n",
        "    average_auc_roc /= num_classes\n",
        "    std_error_roc = np.std(auc_roc_list) / np.sqrt(num_classes)\n",
        "    print(f\"Average AUC for ROC Curve: {average_auc_roc:.2f} ± {std_error_roc:.2f}\")\n",
        "\n",
        "    plt.title('ROC Curve - Arabidopsis thaliana', fontsize=16)\n",
        "    plt.xlabel('False Positive Rate', fontsize=14, labelpad=15)\n",
        "    plt.ylabel('True Positive Rate', fontsize=14, labelpad=15)\n",
        "    plt.grid(False)\n",
        "    plt.text(0.6, 0.1, f\"Avg AUC = {average_auc_roc:.2f} ± {std_error_roc:.2f}\",\n",
        "             transform=plt.gca().transAxes, fontsize=12, color='darkblue')  # AUC annotation\n",
        "    plt.savefig(\"Arabidopsis_thaliana_roc_curve.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Main execution function\n",
        "def main(file_path):\n",
        "    sequences, labels, categories = load_data(file_path)\n",
        "    num_classes = len(categories)\n",
        "    class_weights = calculate_class_weights(labels, num_classes)\n",
        "\n",
        "    dataset = ProteinDataset(sequences, labels)\n",
        "\n",
        "    # Stratified K-Fold Cross-Validation\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # To store results across all folds\n",
        "    all_y_true = []\n",
        "    all_y_probs = []\n",
        "\n",
        "    for fold_num, (train_idx, test_idx) in enumerate(skf.split(np.zeros(len(dataset)), labels)):\n",
        "        print(f\"\\nFold {fold_num + 1}\")\n",
        "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
        "        test_subset = torch.utils.data.Subset(dataset, test_idx)\n",
        "\n",
        "        train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "        test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)\n",
        "\n",
        "        model = CNNLSTMModel(input_size=20, hidden_size=64, num_classes=num_classes).to(device)\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        train_model(model, train_loader, criterion, optimizer, num_epochs=50)\n",
        "\n",
        "        y_true, y_pred, y_probs, loss = evaluate_model(model, test_loader, criterion)\n",
        "\n",
        "        # Collect all true labels and probabilities for metrics\n",
        "        all_y_true.extend(y_true)\n",
        "        all_y_probs.append(y_probs)  # Store probabilities for AUC\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_y_true = np.array(all_y_true)\n",
        "    all_y_probs = np.vstack(all_y_probs)  # Stack probabilities from all folds\n",
        "\n",
        "    # Plot metrics after all folds\n",
        "    plot_metrics(all_y_true, all_y_probs, num_classes)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = '/content/drive/MyDrive/..../chloroplast_40N_5to20_overlap.csv'\n",
        "    main(file_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "MsKTkGYKW8lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##The CNN-LSTM model’s performance on the protein datasets via average metrics (Macro/Weighted Precision,Recall, and F1 Score) with standard errors\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Data preprocessing function\n",
        "def one_hot_encode_sequence(seq, alphabet=\"ACDEFGHIKLMNPQRSTVWY\"):\n",
        "    encoding = np.zeros((len(seq), len(alphabet)), dtype=np.float32)\n",
        "    for i, aa in enumerate(seq):\n",
        "        if aa in alphabet:\n",
        "            encoding[i, alphabet.index(aa)] = 1.0\n",
        "    return encoding\n",
        "\n",
        "# Custom dataset class\n",
        "class ProteinDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.sequences[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "# Model definition with dropout and an additional dense layer\n",
        "class CNNLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, dropout_rate=0.5):\n",
        "        super(CNNLSTMModel, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(hidden_size, 64)  # Additional dense layer\n",
        "        self.fc2 = nn.Linear(64, num_classes)  # Final layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Change to (batch, channels, sequence_length)\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "\n",
        "        x = x.permute(0, 2, 1)  # Change to (batch, sequence_length, channels)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = x[:, -1, :]  # Take the last hidden state\n",
        "\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))  # Apply dropout before the last layer\n",
        "        out = self.fc2(x)\n",
        "        return out\n",
        "\n",
        "# Load data and preprocess\n",
        "def load_data(file_path):\n",
        "    data = pd.read_csv(file_path, header=None, names=[\"sequence\", \"label\"])\n",
        "    sequences = [one_hot_encode_sequence(seq) for seq in data[\"sequence\"]]\n",
        "    labels = data[\"label\"].astype(\"category\").cat.codes\n",
        "    return np.array(sequences), np.array(labels), data[\"label\"].astype(\"category\").cat.categories\n",
        "\n",
        "# Calculate class weights\n",
        "def calculate_class_weights(labels, num_classes):\n",
        "    label_counts = Counter(labels)\n",
        "    total_samples = sum(label_counts.values())\n",
        "    weights = [total_samples / label_counts[i] if i in label_counts else 0.0 for i in range(num_classes)]\n",
        "    return torch.tensor(weights).to(device)\n",
        "\n",
        "# Optional Focal Loss definition\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = nn.CrossEntropyLoss()(inputs, targets)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "        return focal_loss\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=50):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        for sequences, labels in train_loader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate weighted and macro precision, recall, and F1 scores\n",
        "        precision_macro = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        recall_macro = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "        precision_weighted = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        recall_weighted = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
        "        print(f\"Weighted - Precision: {precision_weighted:.4f}, Recall: {recall_weighted:.4f}, F1 Score: {f1_weighted:.4f}\")\n",
        "        print(f\"Macro - Precision: {precision_macro:.4f}, Recall: {recall_macro:.4f}, F1 Score: {f1_macro:.4f}\")\n",
        "\n",
        "# Evaluation function with confusion matrix\n",
        "def evaluate_model(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        all_preds, all_labels = [], []\n",
        "        total_loss = 0.0\n",
        "        for sequences, labels in data_loader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate weighted and macro precision, recall, and F1 scores\n",
        "        precision_macro = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        recall_macro = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "        precision_weighted = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        recall_weighted = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        f1_weighted = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "        # Print metrics\n",
        "        print(f\"Loss: {total_loss:.4f}\")\n",
        "        print(f\"Weighted - Precision: {precision_weighted:.4f}, Recall: {recall_weighted:.4f}, F1 Score: {f1_weighted:.4f}\")\n",
        "        print(f\"Macro - Precision: {precision_macro:.4f}, Recall: {recall_macro:.4f}, F1 Score: {f1_macro:.4f}\")\n",
        "\n",
        "        return all_labels, all_preds\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Main function with multiple-run Stratified K-Fold Cross-Validation\n",
        "def main():\n",
        "    # Load and prepare data\n",
        "    file_path = '/content/drive/MyDrive/..../chloroplast_40N_5to20_overlap.csv'\n",
        "    sequences, labels, unique_labels = load_data(file_path)\n",
        "\n",
        "    # Parameters for multiple runs\n",
        "    num_runs = 3  # Define the number of runs with different seeds\n",
        "    num_folds = 5  # Define the number of folds for each Stratified K-Fold\n",
        "\n",
        "    # Initialize lists to store performance metrics\n",
        "    precision_macro_list, recall_macro_list, f1_macro_list = [], [], []\n",
        "    precision_weighted_list, recall_weighted_list, f1_weighted_list = [], [], []\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        print(f\"\\n--- Run {run + 1} ---\")\n",
        "\n",
        "        # Set a new seed for each run to vary the split\n",
        "        seed = np.random.randint(10000)\n",
        "        skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
        "\n",
        "        # Store performance metrics for each fold in the current run\n",
        "        run_precision_macro, run_recall_macro, run_f1_macro = [], [], []\n",
        "        run_precision_weighted, run_recall_weighted, run_f1_weighted = [], [], []\n",
        "\n",
        "        for fold, (train_idx, test_idx) in enumerate(skf.split(sequences, labels)):\n",
        "            print(f\"\\nFold {fold + 1} (Seed: {seed})\")\n",
        "\n",
        "            # Split data into training and testing sets\n",
        "            train_sequences, test_sequences = sequences[train_idx], sequences[test_idx]\n",
        "            train_labels, test_labels = labels[train_idx], labels[test_idx]\n",
        "\n",
        "            # Stratified Shuffle Split for validation\n",
        "            sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
        "            for train_idx_val, val_idx in sss.split(train_sequences, train_labels):\n",
        "                train_sequences_split, val_sequences = train_sequences[train_idx_val], train_sequences[val_idx]\n",
        "                train_labels_split, val_labels = train_labels[train_idx_val], train_labels[val_idx]\n",
        "\n",
        "            # Create datasets and dataloaders\n",
        "            train_dataset = ProteinDataset(train_sequences_split, train_labels_split)\n",
        "            val_dataset = ProteinDataset(val_sequences, val_labels)\n",
        "            test_dataset = ProteinDataset(test_sequences, test_labels)\n",
        "\n",
        "            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "            val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "            test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "            # Create model, define loss and optimizer\n",
        "            num_classes = len(unique_labels)\n",
        "            model = CNNLSTMModel(input_size=20, hidden_size=64, num_classes=num_classes).to(device)\n",
        "            criterion = nn.CrossEntropyLoss(weight=calculate_class_weights(train_labels_split, num_classes))\n",
        "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "            # Train the model\n",
        "            train_model(model, train_loader, criterion, optimizer, num_epochs=50)\n",
        "\n",
        "            # Evaluate on validation set\n",
        "            print(\"Validation Set:\")\n",
        "            evaluate_model(model, val_loader, criterion)\n",
        "\n",
        "            # Evaluate on test set and store metrics\n",
        "            print(\"Test Set:\")\n",
        "            test_labels, test_preds = evaluate_model(model, test_loader, criterion)\n",
        "\n",
        "            # Calculate fold metrics\n",
        "            precision_macro = precision_score(test_labels, test_preds, average='macro', zero_division=0)\n",
        "            recall_macro = recall_score(test_labels, test_preds, average='macro', zero_division=0)\n",
        "            f1_macro = f1_score(test_labels, test_preds, average='macro', zero_division=0)\n",
        "\n",
        "            precision_weighted = precision_score(test_labels, test_preds, average='weighted', zero_division=0)\n",
        "            recall_weighted = recall_score(test_labels, test_preds, average='weighted', zero_division=0)\n",
        "            f1_weighted = f1_score(test_labels, test_preds, average='weighted', zero_division=0)\n",
        "\n",
        "            # Append fold metrics to the current run's list\n",
        "            run_precision_macro.append(precision_macro)\n",
        "            run_recall_macro.append(recall_macro)\n",
        "            run_f1_macro.append(f1_macro)\n",
        "            run_precision_weighted.append(precision_weighted)\n",
        "            run_recall_weighted.append(recall_weighted)\n",
        "            run_f1_weighted.append(f1_weighted)\n",
        "\n",
        "        # Store average metrics across folds for the current run\n",
        "        precision_macro_list.append(np.mean(run_precision_macro))\n",
        "        recall_macro_list.append(np.mean(run_recall_macro))\n",
        "        f1_macro_list.append(np.mean(run_f1_macro))\n",
        "        precision_weighted_list.append(np.mean(run_precision_weighted))\n",
        "        recall_weighted_list.append(np.mean(run_recall_weighted))\n",
        "        f1_weighted_list.append(np.mean(run_f1_weighted))\n",
        "\n",
        "    # Calculate final average and standard error across all runs\n",
        "    def mean_std_err(metrics):\n",
        "        return np.mean(metrics), np.std(metrics) / np.sqrt(len(metrics))\n",
        "\n",
        "    # Print average metrics with standard errors\n",
        "    print(\"\\n--- Overall Results ---\")\n",
        "    print(f\"Macro Precision: {mean_std_err(precision_macro_list)}\")\n",
        "    print(f\"Macro Recall: {mean_std_err(recall_macro_list)}\")\n",
        "    print(f\"Macro F1 Score: {mean_std_err(f1_macro_list)}\")\n",
        "    print(f\"Weighted Precision: {mean_std_err(precision_weighted_list)}\")\n",
        "    print(f\"Weighted Recall: {mean_std_err(recall_weighted_list)}\")\n",
        "    print(f\"Weighted F1 Score: {mean_std_err(f1_weighted_list)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "zxHs0-IlZLmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#K-mer analysis and identification of common patterns for unsupervised data analysis\n",
        "\n",
        "import pandas as pd\n",
        "from Bio import SeqIO\n",
        "from collections import defaultdict\n",
        "\n",
        "# Function to extract k-mers from a sequence\n",
        "def extract_kmers(seq, k):\n",
        "    return [seq[i:i+k] for i in range(len(seq) - k + 1)]\n",
        "\n",
        "# Function to find common k-mers between each pair of sequences and save results\n",
        "def find_common_kmers(fasta_file, min_k, max_k, output_file):\n",
        "    sequences = []\n",
        "    sequence_names = []\n",
        "\n",
        "    # Read sequences and names from the FASTA file\n",
        "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
        "        sequences.append(str(record.seq))\n",
        "        sequence_names.append(record.id)  # Use the record id as the sequence name\n",
        "\n",
        "    kmer_dict = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    # Collect k-mers for all sequences\n",
        "    for i, seq in enumerate(sequences):\n",
        "        for k in range(min_k, max_k + 1):\n",
        "            kmers = extract_kmers(seq, k)\n",
        "            for kmer in kmers:\n",
        "                kmer_dict[i][(k, kmer)] += 1\n",
        "\n",
        "    # Find common k-mers between each pair of sequences\n",
        "    results = []\n",
        "    for i in range(len(sequences)):\n",
        "        for j in range(i + 1, len(sequences)):\n",
        "            common_kmers = set(kmer_dict[i].keys()) & set(kmer_dict[j].keys())\n",
        "            total_common_kmers = sum(min(kmer_dict[i][kmer], kmer_dict[j][kmer]) for kmer in common_kmers)\n",
        "\n",
        "            # Only store the sequence names and total common k-mers\n",
        "            results.append({\n",
        "                'Sequence1': sequence_names[i],  # Use sequence names\n",
        "                'Sequence2': sequence_names[j],  # Use sequence names\n",
        "                'Total Common K-mers': total_common_kmers\n",
        "            })\n",
        "\n",
        "    # Save results to a CSV file with the selected columns\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Results saved to {output_file}\")\n",
        "\n",
        "# Example usage\n",
        "fasta_file = '/content/drive/MyDrive/..../chloroplast_300 bp up.fasta'  # Update this path as needed\n",
        "min_k = 5  # Minimum k-mer length\n",
        "max_k = 15  # Maximum k-mer length\n",
        "output_file = '/content/drive/MyDrive/..../chloroplast_300N_5-12_kmers-total.csv'  # Output file for common k-mers\n",
        "\n",
        "find_common_kmers(fasta_file, min_k, max_k, output_file)"
      ],
      "metadata": {
        "id": "Ky7rmDOcYk96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clustering and visualization of sequence similarities\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from adjustText import adjust_text\n",
        "\n",
        "# Function to load the CSV file with total common k-mers\n",
        "def load_kmer_data(file_path):\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "# Function to create a distance matrix\n",
        "def create_distance_matrix(df, names):\n",
        "    num_sequences = len(names)\n",
        "    distance_matrix = np.zeros((num_sequences, num_sequences))\n",
        "\n",
        "    for i in range(num_sequences):\n",
        "        for j in range(num_sequences):\n",
        "            if i != j:\n",
        "                # Set distance to be the inverse of the total common k-mers (for clustering purposes)\n",
        "                total_common_kmers = df[(df['Sequence1'] == names[i]) & (df['Sequence2'] == names[j])]['Total Common K-mers']\n",
        "                if not total_common_kmers.empty:\n",
        "                    distance_matrix[i, j] = max(0, np.max(total_common_kmers))  # Avoid negative distances\n",
        "                else:\n",
        "                    distance_matrix[i, j] = 0  # No common k-mers, set distance to 0\n",
        "\n",
        "    return distance_matrix\n",
        "\n",
        "def perform_clustering_and_pca(distance_matrix, names, num_clusters, seed=21):\n",
        "    # Perform K-means clustering\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=seed)\n",
        "    clustering = kmeans.fit_predict(distance_matrix)\n",
        "\n",
        "    # Perform PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_results = pca.fit_transform(distance_matrix)\n",
        "\n",
        "    # Create a DataFrame for plotting\n",
        "    df_pca = pd.DataFrame(pca_results, columns=['PC1', 'PC2'])\n",
        "    df_pca['Cluster'] = clustering\n",
        "    df_pca['Sequence'] = [name.replace(\"5UTR_\", \"\") for name in names]  # Remove \"5UTR_\" from labels\n",
        "\n",
        "    # Plot PCA results\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    scatter = sns.scatterplot(x='PC1', y='PC2', hue='Cluster', palette='tab10', data=df_pca, s=100, edgecolor='k')\n",
        "\n",
        "    # Add labels to data points with initial offset\n",
        "    texts = []\n",
        "    label_space = 0.4  # Initial label space adjustment\n",
        "    for i, txt in enumerate(df_pca['Sequence']):\n",
        "        texts.append(plt.text(df_pca['PC1'].iloc[i] + label_space,\n",
        "                              df_pca['PC2'].iloc[i] + label_space,\n",
        "                              txt, fontsize=12))\n",
        "\n",
        "    # Adjust text labels to avoid overlap\n",
        "    adjust_text(texts,\n",
        "                expand_points=(1.5, 1.5),  # Expand points to give more space\n",
        "                expand_text=(1.5, 1.5),    # Expand text to give more space\n",
        "                arrowprops=dict(arrowstyle='->', color='grey', lw=0.5))\n",
        "\n",
        "    plt.xlabel('Principal Component 1', fontsize=16, labelpad=15)\n",
        "    plt.ylabel('Principal Component 2', fontsize=16, labelpad=15)\n",
        "\n",
        "    # Customize the legend to start from 1\n",
        "    handles, labels = scatter.get_legend_handles_labels()\n",
        "    labels = [f'Cluster {int(label) + 1}' for label in labels]\n",
        "    plt.legend(handles, labels, title='Cluster')\n",
        "\n",
        "    plt.grid(False)\n",
        "    plt.savefig(\"Chlorella vulgaris_Cluster.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Write each cluster and its sequences\n",
        "    for cluster_num in range(num_clusters):\n",
        "        sequences_in_cluster = df_pca[df_pca['Cluster'] == cluster_num]['Sequence'].values\n",
        "        print(f\"Cluster {cluster_num + 1}:\")\n",
        "        for sequence in sequences_in_cluster:\n",
        "            print(f\" - {sequence}\")\n",
        "        print()  # Add space between clusters\n",
        "\n",
        "# Example usage\n",
        "kmer_data_file = '/content/drive/MyDrive/..../chloroplast_300N_5-12_kmers-total.csv'  # Output file from Part 1\n",
        "kmer_data = load_kmer_data(kmer_data_file)\n",
        "\n",
        "# Get unique sequence names\n",
        "sequence_names = pd.concat([kmer_data['Sequence1'], kmer_data['Sequence2']]).unique()\n",
        "\n",
        "# Create distance matrix\n",
        "distance_matrix = create_distance_matrix(kmer_data, sequence_names)\n",
        "\n",
        "# Define the number of clusters for K-means\n",
        "num_clusters = 7\n",
        "\n",
        "# Perform clustering and PCA\n",
        "perform_clustering_and_pca(distance_matrix, sequence_names, num_clusters)\n"
      ],
      "metadata": {
        "id": "KGXXyJ4hYuyf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}